{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<!-- Markdown Cell 0: Notebook Title / Overview -->\n",
        "# **Memory-Robust Few-Shot Test-Time Adaptation on CIFAR-10-C (Q-MemBN+)**\n",
        "\n",
        "This notebook implements my end-to-end pipeline for **robust test-time adaptation (TTA)** on small vision models.  \n",
        "I pretrain a ResNet-18-style backbone on **CIFAR-10**, then adapt it online to **CIFAR-10-C** using my **Q-MemBN+** 'recipe':\n",
        "- **Quantile + Memory BatchNorm** for robust, drift-aware normalization  \n",
        "- **Few-shot support fine-tuning** to align to a new corruption  \n",
        "- **Online entropy + prototype-guided updates** to adapt continuously  \n",
        "\n",
        "**Datasets**\n",
        "- CIFAR-10 (clean source)  \n",
        "- CIFAR-10-C (15 corruptions × 5 severities)\n",
        "\n",
        "A **Pipeline Flowchart** is shown below (double click this cell and click on the link to view in more detail).\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1BnJLXmTVz0GxjURdcOgZOB_b4DXiwrHP\" width=\"1200\">"
      ],
      "metadata": {
        "id": "uejYlTX27Rc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 1: Setup & Data Acquisition**\n",
        "### **Downloading CIFAR-10-C**\n",
        "\n",
        "CIFAR-10 is pulled automatically by torchvision, but **CIFAR-10-C must be fetched manually**.  \n",
        "To do that, I create a local data directory, download the official Zenodo tarball, and then unpack it.  \n",
        "By keeping the raw corruption files intact, I skip re-downloading the dataset when I slice it by **corruption type** and **severity** later.\n"
      ],
      "metadata": {
        "id": "AsxV_wam7oTN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf4R4KvBbP5H",
        "outputId": "1818ea93-7109-4a23-80de-70cb2aac8bf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-23 21:46:05--  https://zenodo.org/api/records/2535967/files/CIFAR-10-C.tar/content\n",
            "Resolving zenodo.org (zenodo.org)... 137.138.52.235, 188.185.48.75, 188.185.43.153, ...\n",
            "Connecting to zenodo.org (zenodo.org)|137.138.52.235|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2918471680 (2.7G) [application/octet-stream]\n",
            "Saving to: ‘/content/data/CIFAR-10-C.tar’\n",
            "\n",
            "/content/data/CIFAR 100%[===================>]   2.72G  20.9MB/s    in 2m 32s  \n",
            "\n",
            "2025-11-23 21:48:38 (18.3 MB/s) - ‘/content/data/CIFAR-10-C.tar’ saved [2918471680/2918471680]\n",
            "\n",
            "brightness.npy\t       gaussian_noise.npy    saturate.npy\n",
            "contrast.npy\t       glass_blur.npy\t     shot_noise.npy\n",
            "defocus_blur.npy       impulse_noise.npy     snow.npy\n",
            "elastic_transform.npy  jpeg_compression.npy  spatter.npy\n",
            "fog.npy\t\t       labels.npy\t     speckle_noise.npy\n",
            "frost.npy\t       motion_blur.npy\t     zoom_blur.npy\n",
            "gaussian_blur.npy      pixelate.npy\n"
          ]
        }
      ],
      "source": [
        "# CIFAR-10-C Dataset Download From Zenodo\n",
        "# CIFAR-10 (without the C for Corruption) is downloaded by PyTorch automatically, so that's good.\n",
        "!mkdir -p /content/data\n",
        "\n",
        "!wget -O /content/data/CIFAR-10-C.tar \\\n",
        "  https://zenodo.org/api/records/2535967/files/CIFAR-10-C.tar/content\n",
        "\n",
        "!tar -xf /content/data/CIFAR-10-C.tar -C /content/data/\n",
        "\n",
        "!ls /content/data/CIFAR-10-C"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- Markdown Cell 2: For Code Cell 1 -->\n",
        "### **Extracting and Verifying the Dataset**\n",
        "\n",
        "I extract the tar archive into my working `data/` folder and list its contents to confirm that they match the standard layout.  \n",
        "This prevents me from encountering silent path issues during an expensive training and adaptation process.\n"
      ],
      "metadata": {
        "id": "wespwhxl718K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Bs6D4ARePHo",
        "outputId": "03f5f5f8-c4f0-40c1-e787-b52ebef2c860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "brightness.npy\t       gaussian_noise.npy    saturate.npy\n",
            "contrast.npy\t       glass_blur.npy\t     shot_noise.npy\n",
            "defocus_blur.npy       impulse_noise.npy     snow.npy\n",
            "elastic_transform.npy  jpeg_compression.npy  spatter.npy\n",
            "fog.npy\t\t       labels.npy\t     speckle_noise.npy\n",
            "frost.npy\t       motion_blur.npy\t     zoom_blur.npy\n",
            "gaussian_blur.npy      pixelate.npy\n"
          ]
        }
      ],
      "source": [
        "# CIFAR-10-C Dataset Extraction\n",
        "!mkdir -p /content/data\n",
        "\n",
        "!tar -xf /content/data/CIFAR-10-C.tar -C /content/data/\n",
        "\n",
        "!ls /content/data/CIFAR-10-C"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 2: Imports & Environment**\n",
        "### **Libraries I Rely On**\n",
        "\n",
        "This cell pulls in:\n",
        "- **PyTorch + torchvision** for models, transforms, and dataloaders  \n",
        "- **NumPy / Python stdlib** for stats, randomness, and utilities  \n",
        "- Small helper types for clear result displayment and de-bugging.\n",
        "\n",
        "I have centralized the imports, and you can read the notebook like the clean pipeline it is.\n"
      ],
      "metadata": {
        "id": "h2L5PnEG8cf0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKjTjrvF_P6D"
      },
      "outputs": [],
      "source": [
        "# Imports.\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, random_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reproducibility Utilities**\n",
        "\n",
        "To compare my adaptation methods fairly, I set **all random seeds** (Python, NumPy, PyTorch CPU/GPU).  \n",
        "This keeps their training curves and TTA outcomes stable across reruns.\n"
      ],
      "metadata": {
        "id": "-0xU3MuT-QCH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9HtVPiM_fEA"
      },
      "outputs": [],
      "source": [
        "# Utilities.\n",
        "def set_global_seed(seed: int = 42) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 3: Q-MemBN Core**\n",
        "### **Quantile + Memory BatchNorm (Q-MemBN)**\n",
        "\n",
        "This is the heart of my method. I replace the standard BN with a layer that:\n",
        "- Uses **median + IQR** instead of mean/variance → robust to outliers & heavy corruption.\n",
        "- Maintains a **FIFO memory** of recent stats → smooths online updates.\n",
        "- Can switch between **source mode** (frozen) and **adapt mode** (streaming).  \n",
        "\n",
        "My helper functions let me:\n",
        "- Toggle adaptation and memory usage globally.\n",
        "- Capture “source” robust stats after Stage-I alignment.  \n",
        "- Reset stats if a drift event is detected.  \n",
        "\n",
        "> **Design Purpose:** make normalization robust, fast, and safe under non-stationary corruptions."
      ],
      "metadata": {
        "id": "5ZQkAsgM-vvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-MemBN (Quantile + Memory BatchNorm)\n",
        "\n",
        "class   QMemBatchNorm2d ( nn.Module ):\n",
        "\n",
        "    # Q-MemBN uses medians + IQR, and keeps a small memory of past stats.\n",
        "    def __init__ ( self ,\n",
        "                   num_features : int ,\n",
        "                   eps : float = 1e-5 ,\n",
        "                   memory_size : int = 16 ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_features = num_features\n",
        "        self.eps          = eps\n",
        "        self.memory_size  = memory_size\n",
        "\n",
        "        # The learnable affine params.\n",
        "        self.weight = nn.Parameter( torch.ones ( num_features ) )\n",
        "        self.bias   = nn.Parameter( torch.zeros( num_features ) )\n",
        "\n",
        "        # Stored “source” stats that I later reset to.\n",
        "        self.register_buffer( \"source_median\" , torch.zeros( num_features ) )\n",
        "        self.register_buffer( \"source_iqr\"    , torch.ones ( num_features ) )\n",
        "\n",
        "        # The FIFO memory for recent medians / IQRs.\n",
        "        self.register_buffer( \"memory_medians\" , torch.zeros( memory_size , num_features ) )\n",
        "        self.register_buffer( \"memory_iqrs\"    , torch.ones ( memory_size , num_features ) )\n",
        "\n",
        "        self.memory_filled : int = 0\n",
        "        self.memory_index  : int = 0\n",
        "\n",
        "        # Controls whether I fuse batch stats with memory.\n",
        "        self.use_memory : bool = False\n",
        "        self.adapt_mode : bool = False\n",
        "\n",
        "\n",
        "    def forward ( self , x : torch.Tensor ) -> torch.Tensor:\n",
        "\n",
        "        if   x.dim() != 4 :\n",
        "            raise ValueError( \"NCHW format input only!\" )\n",
        "\n",
        "        N , C , H , W = x.shape\n",
        "\n",
        "        # Per-channel flattening, to compute robust stats.\n",
        "        x_flat = ( x.permute(1,0,2,3)\n",
        "                     .contiguous()\n",
        "                     .view( C , -1 ) )\n",
        "\n",
        "        batch_median = x_flat.median( dim = 1 ).values\n",
        "        q1           = x_flat.quantile( 0.25 , dim = 1 )\n",
        "        q3           = x_flat.quantile( 0.75 , dim = 1 )\n",
        "        batch_iqr    = ( q3 - q1 ).clamp_min( self.eps )\n",
        "\n",
        "        # Fuse the memory stats only when I use them (efficient).\n",
        "        if self.use_memory  and  self.memory_filled > 0 :\n",
        "            mem_med = self.memory_medians[:self.memory_filled].mean( dim = 0 )\n",
        "            mem_iqr = self.memory_iqrs   [:self.memory_filled].mean( dim = 0 )\n",
        "\n",
        "            median  = 0.5 * batch_median + 0.5 * mem_med\n",
        "            iqr     = 0.5 * batch_iqr    + 0.5 * mem_iqr\n",
        "        else :\n",
        "            median  = batch_median\n",
        "            iqr     = batch_iqr\n",
        "\n",
        "        # Normalizes to then apply gamma / beta.\n",
        "        x_norm = ( x - median.view(1,C,1,1) ) / ( iqr.view(1,C,1,1) + self.eps )\n",
        "        out    = self.weight.view(1,C,1,1) * x_norm   +   self.bias.view(1,C,1,1)\n",
        "\n",
        "        # I record memory during training or adaptation.\n",
        "        if self.training or self.adapt_mode :\n",
        "            with torch.no_grad():\n",
        "                if self.memory_size > 0 :\n",
        "                    idx = self.memory_index\n",
        "                    self.memory_medians[idx].copy_( batch_median )\n",
        "                    self.memory_iqrs   [idx].copy_( batch_iqr    )\n",
        "                    self.memory_index   = ( idx + 1 ) % self.memory_size\n",
        "                    self.memory_filled  = min( self.memory_filled + 1 , self.memory_size )\n",
        "\n",
        "        return out\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_memory_stats(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Return aggregated memory median and IQR (mean over the FIFO).\"\"\"\n",
        "        if self.memory_filled == 0:\n",
        "            # If empty, fall back to source stats (or defaults)\n",
        "            return self.source_median, self.source_iqr\n",
        "        med = self.memory_medians[:self.memory_filled].mean(dim=0)\n",
        "        iqr = self.memory_iqrs[:self.memory_filled].mean(dim=0)\n",
        "        return med, iqr\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def reset_to_source(self) -> None:\n",
        "        \"\"\"Reset memory to the source stats (used by drift detection resets).\"\"\"\n",
        "        self.memory_medians.copy_(self.source_median.unsqueeze(0).repeat(self.memory_size, 1))\n",
        "        self.memory_iqrs.copy_(self.source_iqr.unsqueeze(0).repeat(self.memory_size, 1))\n",
        "        self.memory_filled = self.memory_size\n",
        "        self.memory_index = 0\n",
        "\n",
        "\n",
        "def set_qmem_adapt_mode(model: nn.Module,\n",
        "                        adapt: bool,\n",
        "                        use_memory: bool = True) -> None:\n",
        "    \"\"\"\n",
        "    Turn adaptation mode on/off for all QMemBatchNorm2d layers.\n",
        "    \"\"\"\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, QMemBatchNorm2d):\n",
        "            m.adapt_mode = adapt\n",
        "            m.use_memory = use_memory\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def capture_bn_source_stats(model: nn.Module) -> None:\n",
        "    # Capture current Q-MemBN memory stats as \"source\" stats, to be used.\n",
        "    # as a safe reset point during drift.\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, QMemBatchNorm2d):\n",
        "            med, iqr = m.get_memory_stats()\n",
        "            m.source_median.copy_(med.detach())\n",
        "            m.source_iqr.copy_(iqr.detach())\n",
        "            m.reset_to_source()\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def reset_bn_stats(model: nn.Module) -> None:\n",
        "\n",
        "    # Reset all Q-MemBN layers to their stored source stats (used on drift).\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, QMemBatchNorm2d):\n",
        "            m.reset_to_source()"
      ],
      "metadata": {
        "id": "WbD5MyYQuh39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 4: Backbone Network**\n",
        "### **ResNet-18 with Q-MemBN**\n",
        "\n",
        "I implement a lightweight **ResNet-18-style** backbone where each BN is swapped for Q-MemBN.  \n",
        "The stack layout stays faithful to ResNet so any improvements (fingers crossed) come from my adaptation, not some architecture trick.\n",
        "\n",
        "Key choices:\n",
        "- Small model to match “compute-poor deployment” settings.  \n",
        "- Feature extractor exposed ( `forward_features` ) for prototype learning later (what I tried doing on my MSE's MCQs).  "
      ],
      "metadata": {
        "id": "yw56DUaS_Q-V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9MuntZD_vgl"
      },
      "outputs": [],
      "source": [
        "# The ResNet-18 (style) backbone (w. Q-MemBN).\n",
        "\n",
        "class BasicBlock ( nn.Module ) :\n",
        "\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__ ( self ,\n",
        "                   in_planes : int ,\n",
        "                   planes    : int ,\n",
        "                   stride    : int = 1 ,\n",
        "                   norm_layer = QMemBatchNorm2d ) :\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d( in_planes , planes ,\n",
        "                                kernel_size = 3 ,\n",
        "                                stride      = stride ,\n",
        "                                padding     = 1 ,\n",
        "                                bias        = False )\n",
        "\n",
        "        self.bn1   = norm_layer( planes )\n",
        "        self.relu  = nn.ReLU( inplace = True )\n",
        "\n",
        "        self.conv2 = nn.Conv2d( planes , planes ,\n",
        "                                kernel_size = 3 ,\n",
        "                                stride      = 1 ,\n",
        "                                padding     = 1 ,\n",
        "                                bias        = False )\n",
        "\n",
        "        self.bn2   = norm_layer( planes )\n",
        "\n",
        "        self.downsample = None\n",
        "        if stride != 1  or  in_planes != planes :\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d( in_planes , planes ,\n",
        "                           kernel_size = 1 ,\n",
        "                           stride      = stride ,\n",
        "                           bias        = False ),\n",
        "                norm_layer( planes )\n",
        "            )\n",
        "\n",
        "\n",
        "    def forward ( self , x : torch.Tensor ) -> torch.Tensor :\n",
        "\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1( x )\n",
        "        out = self.bn1 ( out )\n",
        "        out = self.relu( out )\n",
        "\n",
        "        out = self.conv2( out )\n",
        "        out = self.bn2 ( out )\n",
        "\n",
        "        if self.downsample is not None :\n",
        "            identity = self.downsample( x )\n",
        "\n",
        "        out = out + identity\n",
        "        out = self.relu( out )\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "# I use this as a small ResNet-18 variant for 32 x 32 images with Q-MemBN everywhere.\n",
        "class QMemResNet18 ( nn.Module ) :\n",
        "\n",
        "    def __init__ ( self ,\n",
        "                   num_classes : int = 10 ,\n",
        "                   norm_layer  = QMemBatchNorm2d ) :\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d( 3 , 64 ,\n",
        "                                kernel_size = 3 ,\n",
        "                                stride      = 1 ,\n",
        "                                padding     = 1 ,\n",
        "                                bias        = False )\n",
        "\n",
        "        self.bn1  = norm_layer( 64 )\n",
        "        self.relu = nn.ReLU( inplace = True )\n",
        "\n",
        "        self.layer1 = self._make_layer(  64 , blocks = 2 , stride = 1 , norm_layer = norm_layer )\n",
        "        self.layer2 = self._make_layer( 128 , blocks = 2 , stride = 2 , norm_layer = norm_layer )\n",
        "        self.layer3 = self._make_layer( 256 , blocks = 2 , stride = 2 , norm_layer = norm_layer )\n",
        "        self.layer4 = self._make_layer( 512 , blocks = 2 , stride = 2 , norm_layer = norm_layer )\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d( ( 1 , 1 ) )\n",
        "        self.fc      = nn.Linear( 512 , num_classes )\n",
        "\n",
        "\n",
        "    def _make_layer ( self ,\n",
        "                      planes    : int ,\n",
        "                      blocks    : int ,\n",
        "                      stride    : int ,\n",
        "                      norm_layer ) -> nn.Sequential :\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            BasicBlock( self.in_planes , planes ,\n",
        "                        stride     = stride ,\n",
        "                        norm_layer = norm_layer )\n",
        "        )\n",
        "        self.in_planes = planes\n",
        "\n",
        "        for _ in range( 1 , blocks ) :\n",
        "            layers.append(\n",
        "                BasicBlock( self.in_planes , planes ,\n",
        "                            stride     = 1 ,\n",
        "                            norm_layer = norm_layer )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential( *layers )\n",
        "\n",
        "\n",
        "    def forward_features ( self , x : torch.Tensor ) -> torch.Tensor :\n",
        "\n",
        "        x = self.conv1( x )\n",
        "        x = self.bn1 ( x )\n",
        "        x = self.relu( x )\n",
        "\n",
        "        x = self.layer1( x )\n",
        "        x = self.layer2( x )\n",
        "        x = self.layer3( x )\n",
        "        x = self.layer4( x )\n",
        "\n",
        "        x = self.avgpool( x )\n",
        "        x = torch.flatten( x , 1 )\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward ( self , x : torch.Tensor ) -> Tuple[ torch.Tensor , torch.Tensor ] :\n",
        "\n",
        "        feats  = self.forward_features( x )\n",
        "        logits = self.fc( feats )\n",
        "\n",
        "        return logits , feats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 5: Memory for Semantics**\n",
        "### **Prototype Bank**\n",
        "\n",
        "During adaptation, I need stable semantic anchors.  \n",
        "This prototype bank stores **one feature prototype per class** and updates them with momentum.\n",
        "\n",
        "It supports:\n",
        "- **Initialization from support set** (true labels).  \n",
        "- **Pseudo-label updates** from confident stream samples.  \n",
        "- A **consistency mask**: only updates when a sample is near its predicted class prototype.  \n",
        "\n",
        "> This gates learning such that noisy pseudo-labels don’t collapse the model."
      ],
      "metadata": {
        "id": "gtLi3BKc_9p-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e04y25l__vio"
      },
      "outputs": [],
      "source": [
        "# Prototype memory bank (few-shot + pseudo-label updates).\n",
        "\n",
        "class PrototypeBank :\n",
        "\n",
        "    def __init__ ( self ,\n",
        "                   feature_dim  : int ,\n",
        "                   num_classes  : int ,\n",
        "                   device       : torch.device ,\n",
        "                   momentum     : float = 0.1 ) :\n",
        "\n",
        "        self.feature_dim = feature_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.device      = device\n",
        "        self.momentum    = momentum\n",
        "\n",
        "        # I store one prototype per class only.\n",
        "        self.prototypes  = torch.zeros( num_classes , feature_dim , device = device )\n",
        "        self.counts      = torch.zeros( num_classes , dtype = torch.long , device = device )\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def initialize_from_support ( self ,\n",
        "                                  model  : nn.Module ,\n",
        "                                  loader : DataLoader ) -> None :\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        for images , labels in loader :\n",
        "\n",
        "            images = images.to( self.device )\n",
        "            labels = labels.to( self.device )\n",
        "\n",
        "            logits , feats = model( images )\n",
        "\n",
        "            for c in range( self.num_classes ) :\n",
        "\n",
        "                mask = ( labels == c )\n",
        "                if not mask.any() :\n",
        "                    continue\n",
        "\n",
        "                mean_feat = feats[ mask ].mean( dim = 0 )\n",
        "\n",
        "                if self.counts[ c ] == 0 :\n",
        "                    self.prototypes[ c ] = mean_feat\n",
        "                else :\n",
        "                    self.prototypes[ c ] = ( 1.0 - self.momentum ) * self.prototypes[ c ]  \\\n",
        "                                           + self.momentum * mean_feat\n",
        "\n",
        "                self.counts[ c ] += mask.sum()\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update ( self ,\n",
        "                 features : torch.Tensor ,\n",
        "                 labels   : torch.Tensor ) -> None :\n",
        "\n",
        "        # I update the class prototypes using the newer, more confident samples here.\n",
        "        for c in range( self.num_classes ) :\n",
        "\n",
        "            mask = ( labels == c )\n",
        "            if not mask.any() :\n",
        "                continue\n",
        "\n",
        "            mean_feat = features[ mask ].mean( dim = 0 )\n",
        "\n",
        "            self.prototypes[ c ] = ( 1.0 - self.momentum ) * self.prototypes[ c ]  \\\n",
        "                                   + self.momentum * mean_feat\n",
        "\n",
        "            self.counts[ c ] += mask.sum()\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def consistency_mask ( self ,\n",
        "                           features         : torch.Tensor ,\n",
        "                           predicted_labels : torch.Tensor ,\n",
        "                           distance_threshold : float = 2.0 ) -> torch.Tensor :\n",
        "\n",
        "        # If the prototypes are empty, I reject everything (for safety).\n",
        "        if ( self.counts == 0 ).all() :\n",
        "            return torch.zeros( predicted_labels.shape[0] ,\n",
        "                                dtype = torch.bool ,\n",
        "                                device = self.device )\n",
        "\n",
        "        dists = torch.cdist( features , self.prototypes ) # (N, C)\n",
        "\n",
        "        nearest_dist  , nearest_class = torch.min( dists , dim = 1 )\n",
        "\n",
        "        mask = ( nearest_class == predicted_labels )  \\\n",
        "               & ( nearest_dist  <= distance_threshold )\n",
        "\n",
        "        return mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- Markdown Cell 8: For Code Cell 7 -->\n",
        "### **Drift Detection**\n",
        "\n",
        "Adaptation can go wrong if the stream shifts abruptly. So, I detect drift by comparing the **latest BN median** to the **memory median**, scaled by memory IQR. If the normalized shift crosses a threshold, I treat it as drift and later **reset BN stats**.  \n",
        "\n",
        "This gives Q-MemBN+ a 'seatbelt' against catastrophic online updates; updates that can down a small-vison model (e.g., Drone).\n"
      ],
      "metadata": {
        "id": "uFEdEW7MAgkX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxW_1gok_vku"
      },
      "outputs": [],
      "source": [
        "# Drift Detection (BN memory + entropy).\n",
        "\n",
        "class DriftDetector :\n",
        "\n",
        "    # Combines the entropy + BN stuff to detect drift.\n",
        "    def __init__ ( self ,\n",
        "                   median_delta_multiplier : float = 3.0 ,\n",
        "                   min_entropy             : float = 1.0 ) :\n",
        "\n",
        "        self.median_delta_multiplier = median_delta_multiplier\n",
        "        self.min_entropy             = min_entropy\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def detect ( self ,\n",
        "                 model             : nn.Module ,\n",
        "                 batch_confidences : torch.Tensor ) -> bool :\n",
        "\n",
        "        # Here we compute the mean prediction entropy for the batch.\n",
        "        probs = batch_confidences\n",
        "        H     = -( probs.clamp_min(1e-8) * probs.clamp_min(1e-8).log() ).sum( dim = 1 )\n",
        "        mean_entropy = H.mean().item()\n",
        "\n",
        "        if mean_entropy < self.min_entropy :\n",
        "            return False\n",
        "\n",
        "        median_shift_norms : List[ float ] = []\n",
        "\n",
        "        for m in model.modules() :\n",
        "\n",
        "            if not isinstance( m , QMemBatchNorm2d ) :\n",
        "                continue\n",
        "            if m.memory_filled <= 0 :\n",
        "                continue\n",
        "\n",
        "            mem_med , mem_iqr = m.get_memory_stats()\n",
        "\n",
        "            # This approximates the batch median (most recent one) from the FIFO tail.\n",
        "            last_idx   = ( m.memory_index - 1 ) % max( 1 , m.memory_size )\n",
        "            batch_med  = m.memory_medians[ last_idx ]\n",
        "\n",
        "            delta      = ( batch_med - mem_med ).abs()\n",
        "            scaled     = delta / ( mem_iqr + 1e-5 )\n",
        "\n",
        "            median_shift_norms.append( scaled.mean().item() )\n",
        "\n",
        "        if not median_shift_norms :\n",
        "            return False\n",
        "\n",
        "        avg_scaled_shift = float( np.mean( median_shift_norms ) )\n",
        "        return avg_scaled_shift > self.median_delta_multiplier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 6: Datasets & Loaders**\n",
        "### **CIFAR-10-C Wrapper + Few-Shot Split**\n",
        "\n",
        "Here, I define a clean wrapper for CIFAR-10-C that slices **(corruption, severity)** into a normal PyTorch dataset.\n",
        "\n",
        "Then, I build:\n",
        "- **Support subset**: `shots_per_class` , or labeled samples per class.\n",
        "- **Stream subset**: the remaining unlabeled data for online TTA.  \n",
        "\n",
        "The transforms use CIFAR-10 normalization so that the source and target stay compatible.\n"
      ],
      "metadata": {
        "id": "HSPNaKPIA44z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xeqo0pN_-_f"
      },
      "outputs": [],
      "source": [
        "# My Datasets: CIFAR-10 and CIFAR-10-C.\n",
        "\n",
        "CIFAR_MEAN = ( 0.4914 , 0.4822 , 0.4465 )\n",
        "CIFAR_STD  = ( 0.2470 , 0.2435 , 0.2616 )\n",
        "\n",
        "\n",
        "class CIFAR10C ( Dataset ) :\n",
        "\n",
        "    # A wrapper that slices one corruption and one severity level.\n",
        "    def __init__ ( self ,\n",
        "                   root       : str ,\n",
        "                   corruption : str ,\n",
        "                   severity   : int = 5 ,\n",
        "                   transform  = None ) :\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.root       = root\n",
        "        self.corruption = corruption\n",
        "        self.severity   = severity\n",
        "        self.transform  = transform\n",
        "\n",
        "        data_path   = os.path.join( root , \"CIFAR-10-C\" , f\"{corruption}.npy\" )\n",
        "        labels_path = os.path.join( root , \"CIFAR-10-C\" , \"labels.npy\" )\n",
        "\n",
        "        self.data   = np.load( data_path ) # info: (50000, 32, 32, 3).\n",
        "        self.labels = np.load( labels_path )\n",
        "\n",
        "        assert 1 <= severity <= 5\n",
        "\n",
        "        n = self.data.shape[0] // 5\n",
        "        st = ( severity - 1 ) * n\n",
        "        en = severity * n\n",
        "\n",
        "        self.data   = self.data  [ st : en ]\n",
        "        self.labels = self.labels[ st : en ]\n",
        "\n",
        "\n",
        "    def __len__ ( self ) -> int :\n",
        "        return len( self.data )\n",
        "\n",
        "\n",
        "    def __getitem__ ( self , idx : int ) -> Tuple[ torch.Tensor , int ] :\n",
        "\n",
        "        img = self.data[ idx ]\n",
        "        img = Image.fromarray( img.astype( np.uint8 ) )\n",
        "\n",
        "        if self.transform is not None :\n",
        "            img = self.transform( img )\n",
        "\n",
        "        label = int( self.labels[ idx ] )\n",
        "        return img , label\n",
        "\n",
        "\n",
        "\n",
        "def get_cifar10_dataloaders ( root        : str ,\n",
        "                              batch_size  : int = 128 ,\n",
        "                              num_workers : int = 2 ) -> Tuple[ Dataset , Dataset , DataLoader , DataLoader ] :\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop( 32 , padding = 4 ),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize( CIFAR_MEAN , CIFAR_STD ),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize( CIFAR_MEAN , CIFAR_STD ),\n",
        "    ])\n",
        "\n",
        "    train_ds = datasets.CIFAR10( root = root ,\n",
        "                                 train = True ,\n",
        "                                 download = True ,\n",
        "                                 transform = transform_train )\n",
        "\n",
        "    test_ds  = datasets.CIFAR10( root = root ,\n",
        "                                 train = False ,\n",
        "                                 download = True ,\n",
        "                                 transform = transform_test )\n",
        "\n",
        "    train_loader = DataLoader( train_ds ,\n",
        "                               batch_size = batch_size ,\n",
        "                               shuffle    = True ,\n",
        "                               num_workers = num_workers )\n",
        "\n",
        "    test_loader  = DataLoader( test_ds ,\n",
        "                               batch_size = batch_size ,\n",
        "                               shuffle    = False ,\n",
        "                               num_workers = num_workers )\n",
        "\n",
        "    return train_ds , test_ds , train_loader , test_loader\n",
        "\n",
        "\n",
        "\n",
        "def get_cifar10c_loader ( root        : str ,\n",
        "                          corruption  : str = \"gaussian_noise\" ,\n",
        "                          severity    : int = 5 ,\n",
        "                          batch_size  : int = 64 ,\n",
        "                          num_workers : int = 2 ,\n",
        "                          shuffle     : bool = False ) -> Tuple[ Dataset , DataLoader ] :\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize( CIFAR_MEAN , CIFAR_STD ),\n",
        "    ])\n",
        "\n",
        "    dataset = CIFAR10C( root       = root ,\n",
        "                        corruption = corruption ,\n",
        "                        severity   = severity ,\n",
        "                        transform  = transform )\n",
        "\n",
        "    loader  = DataLoader( dataset ,\n",
        "                          batch_size  = batch_size ,\n",
        "                          shuffle     = shuffle ,\n",
        "                          num_workers = num_workers )\n",
        "\n",
        "    return dataset , loader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_support_and_stream_subsets ( dataset        : Dataset ,\n",
        "                                       num_classes    : int ,\n",
        "                                       shots_per_class : int = 5 ) -> Tuple[ Subset , Subset ] :\n",
        "\n",
        "    # I split the things into low-shot labeled support + an unlabeled adaptation stream.\n",
        "    indices_per_class = { c : [] for c in range( num_classes ) }\n",
        "\n",
        "    all_indices = list( range( len( dataset ) ) )\n",
        "    random.shuffle( all_indices )\n",
        "\n",
        "    for idx in all_indices :\n",
        "\n",
        "        _ , label = dataset[ idx ]\n",
        "\n",
        "        if len( indices_per_class[ label ] ) < shots_per_class :\n",
        "            indices_per_class[ label ].append( idx )\n",
        "\n",
        "        if all( len(v) >= shots_per_class for v in indices_per_class.values() ) :\n",
        "            break\n",
        "\n",
        "    support_indices : List[int] = []\n",
        "    for c in range( num_classes ) :\n",
        "        support_indices.extend( indices_per_class[ c ] )\n",
        "\n",
        "    support_indices = sorted( support_indices )\n",
        "\n",
        "    full_set    = set( range( len( dataset ) ) )\n",
        "    support_set = set( support_indices )\n",
        "\n",
        "    stream_indices = sorted( list( full_set - support_set ) )\n",
        "\n",
        "    support_subset = Subset( dataset , support_indices )\n",
        "    stream_subset  = Subset( dataset , stream_indices )\n",
        "\n",
        "    return support_subset , stream_subset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- Markdown Cell 10: For Code Cell 9 -->\n",
        "## **Section 7: Metrics**\n",
        "### **Interpretable Evaluation**\n",
        "\n",
        "I compute:\n",
        "- Accuracy  \n",
        "- Macro Precision / Recall / F1 (class-balanced)  \n",
        "- RMSE over probabilities (calibration feel)\n",
        "\n",
        "I chose these metrics because they summarize both **correctness** and **confidence behavior** under corruption.\n"
      ],
      "metadata": {
        "id": "o6b1jMMHBMBl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDNTbZiBAJ7w"
      },
      "outputs": [],
      "source": [
        "# My Metrics: accuracy, precision, recall, F1, RMSE.\n",
        "\n",
        "def compute_classification_metrics ( logits : torch.Tensor ,\n",
        "                                     targets : torch.Tensor ,\n",
        "                                     num_classes : int ) -> Dict[ str , float ] :\n",
        "\n",
        "    # Computes the CORE classification metrics and also the RMSE (over probabilities ofc.).\n",
        "    with torch.no_grad() :\n",
        "\n",
        "        probs = F.softmax( logits , dim = 1 )\n",
        "        preds = probs.argmax( dim = 1 )\n",
        "\n",
        "        total    = targets.numel()\n",
        "        correct  = ( preds == targets ).sum().item()\n",
        "        accuracy = correct / total\n",
        "\n",
        "        TP = torch.zeros( num_classes )\n",
        "        FP = torch.zeros( num_classes )\n",
        "        FN = torch.zeros( num_classes )\n",
        "\n",
        "        for c in range( num_classes ) :\n",
        "            TP[ c ] = ( ( preds == c ) & ( targets == c ) ).sum()\n",
        "            FP[ c ] = ( ( preds == c ) & ( targets != c ) ).sum()\n",
        "            FN[ c ] = ( ( preds != c ) & ( targets == c ) ).sum()\n",
        "\n",
        "        precision_per_class = TP / ( TP + FP + 1e-8 )\n",
        "        recall_per_class    = TP / ( TP + FN + 1e-8 )\n",
        "        f1_per_class        = 2 * precision_per_class * recall_per_class \\\n",
        "                              / ( precision_per_class + recall_per_class + 1e-8 )\n",
        "\n",
        "        precision_macro = precision_per_class.mean().item()\n",
        "        recall_macro    = recall_per_class.mean().item()\n",
        "        f1_macro        = f1_per_class.mean().item()\n",
        "\n",
        "        # Measures the RMSE between the predicted probabilities and then the one-hot labels.\n",
        "        y_one_hot = F.one_hot( targets , num_classes = num_classes ).float()\n",
        "        rmse      = torch.sqrt( ( ( probs - y_one_hot ) ** 2 ).mean() ).item()\n",
        "\n",
        "        return {\n",
        "            \"accuracy\"        : accuracy ,\n",
        "            \"precision_macro\" : precision_macro ,\n",
        "            \"recall_macro\"    : recall_macro ,\n",
        "            \"f1_macro\"        : f1_macro ,\n",
        "            \"rmse\"            : rmse ,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Train/Eval Routines**\n",
        "\n",
        "This is a standard supervised loop:\n",
        "- `train_one_epoch` : means cross-entropy training on clean CIFAR-10.  \n",
        "- `evaluate` : means identical logic in `no_grad` mode.  \n",
        "\n",
        "I log full-dataset metrics each epoch so my progress remains interpretable later."
      ],
      "metadata": {
        "id": "LEncz4NfBXwV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqYZpgtDAPVW"
      },
      "outputs": [],
      "source": [
        "# A normal training and evaluation loop.\n",
        "\n",
        "def train_one_epoch ( model      : nn.Module ,\n",
        "                      loader     : DataLoader ,\n",
        "                      optimizer  : torch.optim.Optimizer ,\n",
        "                      device     : torch.device ,\n",
        "                      num_classes : int ) -> Tuple[ float , Dict[ str , float ] ] :\n",
        "\n",
        "    # I run one full training pass over the dataloader.\n",
        "    model.train()\n",
        "\n",
        "    total_loss  = 0.0\n",
        "    all_logits  = []\n",
        "    all_targets = []\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for images , labels in loader :\n",
        "\n",
        "        images = images.to( device )\n",
        "        labels = labels.to( device )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits , feats = model( images )\n",
        "        loss           = criterion( logits , labels )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * labels.size( 0 )\n",
        "\n",
        "        all_logits.append ( logits.detach().cpu()  )\n",
        "        all_targets.append( labels.detach().cpu() )\n",
        "\n",
        "    all_logits  = torch.cat( all_logits  , dim = 0 )\n",
        "    all_targets = torch.cat( all_targets , dim = 0 )\n",
        "\n",
        "    metrics  = compute_classification_metrics( all_logits , all_targets , num_classes )\n",
        "    avg_loss = total_loss / len( loader.dataset )\n",
        "\n",
        "    return avg_loss , metrics\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate ( model       : nn.Module ,\n",
        "               loader      : DataLoader ,\n",
        "               device      : torch.device ,\n",
        "               num_classes : int ) -> Tuple[ float , Dict[ str , float ] ] :\n",
        "\n",
        "    # And I evaluate without gradient tracking (saves resources, not needed for prediction / performance stuff).\n",
        "    model.eval()\n",
        "\n",
        "    total_loss  = 0.0\n",
        "    all_logits  = []\n",
        "    all_targets = []\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for images , labels in loader :\n",
        "\n",
        "        images = images.to( device )\n",
        "        labels = labels.to( device )\n",
        "\n",
        "        logits , feats = model( images )\n",
        "        loss           = criterion( logits , labels )\n",
        "\n",
        "        total_loss += loss.item() * labels.size( 0 )\n",
        "\n",
        "        all_logits.append ( logits.cpu()  )\n",
        "        all_targets.append( labels.cpu() )\n",
        "\n",
        "    all_logits  = torch.cat( all_logits  , dim = 0 )\n",
        "    all_targets = torch.cat( all_targets , dim = 0 )\n",
        "\n",
        "    metrics  = compute_classification_metrics( all_logits , all_targets , num_classes )\n",
        "    avg_loss = total_loss / len( loader.dataset )\n",
        "\n",
        "    return avg_loss , metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 8: Source Training (Stage-0)**\n",
        "### **Hold-Out Hyperparameter Search + Final Training**\n",
        "\n",
        "Before any adaptation, I am going to need a solid **source model**.  \n",
        "I do a simple 80/20 hold-out search over learning rates and weight decay, pick the best combination of their values, then train fully.\n",
        "\n",
        "Why this way?\n",
        "- Its cheap and practical. Hyperparameter search should never consume the majority of your compute budget.\n",
        "- Prevents over-engineering CV (where l_r and w_d diverge instantly) but still avoids bad defaults."
      ],
      "metadata": {
        "id": "uNBhCBZkB0uD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "err5BiGKASnO"
      },
      "outputs": [],
      "source": [
        "# Simple hyperparameter \"CV\" (hold-out validation) for source training.\n",
        "\n",
        "def hyperparam_search ( train_dataset  : Dataset ,\n",
        "                        num_classes    : int ,\n",
        "                        device         : torch.device ,\n",
        "                        learning_rates : List[ float ] ,\n",
        "                        weight_decays  : List[ float ] ,\n",
        "                        batch_size     : int = 128 ,\n",
        "                        max_epochs     : int = 5 ) -> Dict[ str , float ] :\n",
        "\n",
        "    # I use a good train/val split (not the one I detailed in my M.S.E.!) to pick the best LR & weight decay.\n",
        "    n_total  = len( train_dataset )\n",
        "    n_val    = int( 0.2 * n_total )\n",
        "    n_train  = n_total - n_val\n",
        "\n",
        "    train_subset , val_subset = random_split( train_dataset , [ n_train , n_val ] )\n",
        "\n",
        "    best_config = None\n",
        "    best_acc    = 0.0\n",
        "\n",
        "    for lr in learning_rates :\n",
        "        for wd in weight_decays :\n",
        "\n",
        "            model = QMemResNet18( num_classes = num_classes ).to( device )\n",
        "            optimizer = torch.optim.SGD(\n",
        "                model.parameters() ,\n",
        "                lr       = lr ,\n",
        "                momentum = 0.9 ,\n",
        "                weight_decay = wd\n",
        "            )\n",
        "\n",
        "            train_loader = DataLoader( train_subset ,\n",
        "                                       batch_size = batch_size ,\n",
        "                                       shuffle    = True )\n",
        "\n",
        "            val_loader = DataLoader( val_subset ,\n",
        "                                     batch_size = batch_size ,\n",
        "                                     shuffle    = False )\n",
        "\n",
        "            for epoch in range( max_epochs ) :\n",
        "                train_one_epoch( model , train_loader , optimizer ,\n",
        "                                 device , num_classes )\n",
        "\n",
        "            _ , val_metrics = evaluate( model , val_loader ,\n",
        "                                        device , num_classes )\n",
        "\n",
        "            val_acc = val_metrics[ \"accuracy\" ]\n",
        "\n",
        "            if val_acc > best_acc :\n",
        "                best_acc    = val_acc\n",
        "                best_config = { \"lr\" : lr , \"weight_decay\" : wd }\n",
        "\n",
        "    if best_config is None :\n",
        "        best_config = { \"lr\" : learning_rates[ 0 ] ,\n",
        "                        \"weight_decay\" : weight_decays[ 0 ] }\n",
        "\n",
        "    return best_config\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_source_model ( data_root : str ,\n",
        "                         num_classes : int ,\n",
        "                         device : torch.device ,\n",
        "                         batch_size : int = 128 ,\n",
        "                         epochs : int = 30 ) -> nn.Module :\n",
        "\n",
        "    # Trains the base source model on CIFAR-10 using the now tuned hyperparameters.\n",
        "    train_ds , test_ds , train_loader , test_loader = \\\n",
        "        get_cifar10_dataloaders( data_root , batch_size = batch_size )\n",
        "\n",
        "    best = hyperparam_search(\n",
        "        train_ds ,\n",
        "        num_classes    = num_classes ,\n",
        "        device         = device ,\n",
        "        learning_rates = [ 0.1 , 0.05 , 0.01 ] ,\n",
        "        weight_decays  = [ 5e-4 , 1e-4 ] ,\n",
        "        batch_size     = batch_size ,\n",
        "        max_epochs     = 3 ,\n",
        "    )\n",
        "\n",
        "    print( \"Best hyperparameters for source training:\" , best )\n",
        "\n",
        "    model = QMemResNet18( num_classes = num_classes ).to( device )\n",
        "\n",
        "    optimizer = torch.optim.SGD(\n",
        "        model.parameters() ,\n",
        "        lr          = best[ \"lr\" ] ,\n",
        "        momentum    = 0.9 ,\n",
        "        weight_decay = best[ \"weight_decay\" ] ,\n",
        "    )\n",
        "\n",
        "    for epoch in range( epochs ) :\n",
        "\n",
        "        train_loss , train_metrics = train_one_epoch(\n",
        "            model , train_loader , optimizer , device , num_classes\n",
        "        )\n",
        "\n",
        "        val_loss , val_metrics = evaluate(\n",
        "            model , test_loader , device , num_classes\n",
        "        )\n",
        "\n",
        "        print(\n",
        "            f\"[Source] Epoch {epoch+1}/{epochs} \"\n",
        "            f\"Train loss {train_loss:.4f} acc {train_metrics['accuracy']:.4f} \"\n",
        "            f\"Val loss {val_loss:.4f} acc {val_metrics['accuracy']:.4f}\"\n",
        "        )\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 9: Stage-I Few-Shot Alignment**\n",
        "### **Support-Set Fine-Tuning with Feature Mixing**\n",
        "\n",
        "Given a small labeled support set, I fine-tune:\n",
        "- The **final FC layer**.\n",
        "- Q-MemBN affine parameters (γ/β).\n",
        "\n",
        "I apply **feature mixing** (α-blend) as a tiny regularizer so the model doesn’t overfit the 5-shot labels.  \n",
        "Afterwards, I **capture the aligned robust BN stats** as the new “source” state for Stage-II."
      ],
      "metadata": {
        "id": "zBg2puS4DDOQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgQXsfHsAYlv"
      },
      "outputs": [],
      "source": [
        "# Stage I: Few-shot fine-tuning with feature mixing.\n",
        "\n",
        "def stage1_finetune ( model         : nn.Module ,\n",
        "                      support_loader : DataLoader ,\n",
        "                      device        : torch.device ,\n",
        "                      num_classes   : int ,\n",
        "                      epochs        : int = 5 ,\n",
        "                      lr            : float = 1e-4 ,\n",
        "                      mix_alpha     : float = 0.3 ) -> None :\n",
        "\n",
        "    # Kept the model in training mode but disable adaptation.\n",
        "    model.train()\n",
        "    set_qmem_adapt_mode( model , adapt = False , use_memory = False )\n",
        "\n",
        "    # Updated BN affine params & the final FC layer.\n",
        "    params : List[ nn.Parameter ] = []\n",
        "    for m in model.modules() :\n",
        "        if isinstance( m , QMemBatchNorm2d ) :\n",
        "            params.append( m.weight )\n",
        "            params.append( m.bias )\n",
        "    params.extend( list( model.fc.parameters() ) )\n",
        "\n",
        "    optimizer = torch.optim.AdamW( params , lr = lr )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range( epochs ) :\n",
        "\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for images , labels in support_loader :\n",
        "\n",
        "            images = images.to( device )\n",
        "            labels = labels.to( device )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits , feats = model( images )\n",
        "\n",
        "            # I mixed features to increase the diversity (within the batch).\n",
        "            if images.size( 0 ) > 1 :\n",
        "\n",
        "                perm = torch.randperm( images.size( 0 ) , device = device )\n",
        "                lam  = float( np.random.beta( mix_alpha , mix_alpha ) )\n",
        "\n",
        "                feats_mix  = lam * feats + ( 1.0 - lam ) * feats[ perm ]\n",
        "                logits_mix = model.fc( feats_mix )\n",
        "\n",
        "                labels_a = labels\n",
        "                labels_b = labels[ perm ]\n",
        "\n",
        "                loss_main = criterion( logits     , labels_a )\n",
        "                loss_mix  = lam * criterion( logits_mix , labels_a ) \\\n",
        "                            + ( 1.0 - lam ) * criterion( logits_mix , labels_b )\n",
        "\n",
        "                loss = 0.5 * ( loss_main + loss_mix )\n",
        "\n",
        "            else :\n",
        "                loss = criterion( logits , labels )\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item() * labels.size( 0 )\n",
        "\n",
        "        epoch_loss /= len( support_loader.dataset )\n",
        "        print( f\"[Stage I] Epoch {epoch+1}/{epochs}, loss {epoch_loss:.4f}\" )\n",
        "\n",
        "    # Captures the aligned BN stats as the new source state.\n",
        "    capture_bn_source_stats( model )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 10: Stage-II Online TTA (Q-MemBN+)**\n",
        "### **Entropy + Prototype-Guided Adaptation**\n",
        "\n",
        "Now I adapt on the unlabeled stream, meaning that:\n",
        "- Q-MemBN runs in **adapt + memory mode**.  \n",
        "- Entropy is minimized on a fraction of the predictions, which stabilizes the decision boundaries.\n",
        "- The prototype bank filters pseudo-labels via their distance consistency.  \n",
        "- The drift detector can trigger BN resets if the stream shifts too hard.  \n",
        "\n",
        "This is the “always-on” deployment behavior I’m targeting.\n"
      ],
      "metadata": {
        "id": "rEMiziUODXCL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0aSak7uAcyB"
      },
      "outputs": [],
      "source": [
        "#  Stage II: Q-MemBN+ test-time adaptation (online).\n",
        "\n",
        "def stage2_adapt_stream ( model          : nn.Module ,\n",
        "                          prototype_bank : PrototypeBank ,\n",
        "                          stream_loader  : DataLoader ,\n",
        "                          device         : torch.device ,\n",
        "                          num_classes    : int ,\n",
        "                          lr             : float = 1e-3 ,\n",
        "                          entropy_fraction   : float = 0.5 ,\n",
        "                          distance_threshold : float = 2.0 ,\n",
        "                          max_batches    : Optional[ int ] = None ) -> Dict[ str , float ] :\n",
        "\n",
        "    # Now we switch the model into adaptation mode with memory enabled.\n",
        "    model.train()\n",
        "    set_qmem_adapt_mode( model , adapt = True , use_memory = True )\n",
        "\n",
        "    # Then we update the BN affine params & the classifier head.\n",
        "    params : List[ nn.Parameter ] = []\n",
        "    for m in model.modules() :\n",
        "        if isinstance( m , QMemBatchNorm2d ) :\n",
        "            params.append( m.weight )\n",
        "            params.append( m.bias )\n",
        "    params.extend( list( model.fc.parameters() ) )\n",
        "\n",
        "    optimizer = torch.optim.SGD( params , lr = lr , momentum = 0.9 )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    drift_detector = DriftDetector(\n",
        "        median_delta_multiplier = 3.0 ,\n",
        "        min_entropy             = math.log( num_classes ) * 0.75\n",
        "    )\n",
        "\n",
        "    all_logits  = []\n",
        "    all_targets = []\n",
        "    batch_counter = 0\n",
        "\n",
        "    for images , labels in stream_loader :\n",
        "\n",
        "        images = images.to( device )\n",
        "        labels = labels.to( device )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits , feats = model( images )\n",
        "        probs          = F.softmax( logits , dim = 1 )\n",
        "\n",
        "        entropy = -( probs.clamp_min(1e-8) * probs.clamp_min(1e-8).log() ).sum( dim = 1 )\n",
        "\n",
        "        # I filter out high-entropy predictions (as they're no good for our purposes).\n",
        "        entropy_threshold = entropy_fraction * math.log( num_classes )\n",
        "        low_entropy_mask  = entropy < entropy_threshold\n",
        "\n",
        "        pseudo_labels = probs.argmax( dim = 1 )\n",
        "\n",
        "        # And I only keep the samples consistent with their nearest prototype.\n",
        "        consistency_mask = prototype_bank.consistency_mask(\n",
        "            feats , pseudo_labels , distance_threshold = distance_threshold\n",
        "        )\n",
        "\n",
        "        accepted_mask = low_entropy_mask & consistency_mask\n",
        "\n",
        "        if accepted_mask.any() :\n",
        "\n",
        "            x_conf = images[ accepted_mask ]\n",
        "            y_conf = pseudo_labels[ accepted_mask ]\n",
        "\n",
        "            logits_conf , feats_conf = model( x_conf )\n",
        "            loss = criterion( logits_conf , y_conf )\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Updating the prototype memory with confident samples now.\n",
        "            prototype_bank.update(\n",
        "                feats_conf.detach() ,\n",
        "                y_conf.detach()\n",
        "            )\n",
        "        else :\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # The drift detection resets the BN stats when needed, and when needed only.\n",
        "        if drift_detector.detect( model , probs ) :\n",
        "            print( \"[Stage II] Drift detected: resetting BN stats to source.\" )\n",
        "            reset_bn_stats( model )\n",
        "\n",
        "        all_logits.append ( logits.detach().cpu()  )\n",
        "        all_targets.append( labels.detach().cpu() )\n",
        "\n",
        "        batch_counter += 1\n",
        "        if max_batches is not None and batch_counter >= max_batches :\n",
        "            break\n",
        "\n",
        "    all_logits  = torch.cat( all_logits  , dim = 0 )\n",
        "    all_targets = torch.cat( all_targets , dim = 0 )\n",
        "\n",
        "    metrics = compute_classification_metrics( all_logits , all_targets , num_classes )\n",
        "    print( \"[Stage II] Final metrics on adapted stream:\" , metrics )\n",
        "\n",
        "    return metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 11: Deployment Utilities**\n",
        "### **Saving, Loading, and Single-Image Prediction**\n",
        "\n",
        "Once adapted, I serialize the:\n",
        "- Model weights  \n",
        "- Number of classes\n",
        "\n",
        "I also provide a small helper for **single-image inference**, using the same CIFAR normalization.  \n",
        "This makes my notebook output directly deployable in, say, a dashboard or an API.\n"
      ],
      "metadata": {
        "id": "vcQ0HbEOFppz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRPEans8AhGT"
      },
      "outputs": [],
      "source": [
        "# Deployment: saving / loading and single-image prediction.\n",
        "\n",
        "def save_deployed_model ( model : nn.Module ,\n",
        "                          path  : str ) -> None :\n",
        "\n",
        "    # Saves the adapted model for later (soon) deployment.\n",
        "    os.makedirs( os.path.dirname( path ) , exist_ok = True )\n",
        "\n",
        "    checkpoint = {\n",
        "        \"state_dict\"  : model.state_dict() ,\n",
        "        \"num_classes\" : model.fc.out_features ,\n",
        "    }\n",
        "\n",
        "    torch.save( checkpoint , path )\n",
        "    print( f\"[Deploy] Saved model to {path}\" )\n",
        "\n",
        "\n",
        "\n",
        "def load_deployed_model ( path   : str ,\n",
        "                          device : torch.device ) -> nn.Module :\n",
        "\n",
        "    # Loads a Q-MemResNet18 checkpoint and disables adaptation (bcs. deployment should be deterministic!).\n",
        "    checkpoint   = torch.load( path , map_location = device )\n",
        "    num_classes  = checkpoint.get( \"num_classes\" , 10 )\n",
        "\n",
        "    model = QMemResNet18( num_classes = num_classes ).to( device )\n",
        "    model.load_state_dict( checkpoint[ \"state_dict\" ] )\n",
        "    model.eval()\n",
        "\n",
        "    set_qmem_adapt_mode( model , adapt = False , use_memory = False )\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_single_image ( image_path : str ,\n",
        "                           model_path : str ,\n",
        "                           device_str : str = \"cpu\" ) -> Tuple[ int , float ] :\n",
        "\n",
        "    # Now we load the model and run inference on 1 image.\n",
        "    device = torch.device( device_str )\n",
        "    model  = load_deployed_model( model_path , device = device )\n",
        "\n",
        "    img = Image.open( image_path ).convert( \"RGB\" )\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize( ( 32 , 32 ) ),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize( CIFAR_MEAN , CIFAR_STD ),\n",
        "    ])\n",
        "\n",
        "    tensor = transform( img ).unsqueeze( 0 ).to( device )\n",
        "\n",
        "    logits , feats = model( tensor )\n",
        "    probs          = F.softmax( logits , dim = 1 )\n",
        "\n",
        "    pred       = probs.argmax( dim = 1 ).item()\n",
        "    confidence = probs[ 0 , pred ].item()\n",
        "\n",
        "    return pred , confidence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 12: End-to-End Pipeline: Training and Evaluation**\n",
        "\n",
        "Together, the three code cells below, each constructing their own pipeline to then call functions, etc., as necessary, do the following:\n",
        "- **Train and test QMem-BN's custom backbone on the CIFAR-10 dataset**. (converged at 77% accuracy, is good) (Cell 1)\n",
        "- **Test this custom ResNet-18 backbone on CIFAR-10-C without adaptation**. (65.91% accuracy; this is the baseline) (Cell 2)\n",
        "- **Test this custom ResNet-18 backbone on CIFAR-10-C with clean adaptation (no entropy rules, prototype filtering, etc.)**. (66.78% accuracy; this is the compelte QMem-BN) (Cell 2)\n",
        "- **Test this custom ResNet-18 backbone on CIFAR-10-C with naive (so none of my rules) adaptation**. (66.62% accuracy; so it is not only less safe but also slightly less accurate) (Cell 2)\n",
        "- **Test robustness to 20% poisoned inputs on both QMem-BN with clean adaptation and naive adaptation**. (naive adaptation: **12.20 pp** accuracy drop; clean adaptation: **11.46 pp** accuracy drop; so we are better than naive adaptation at malicious-input resistancy) (Cell 3)\n",
        "- **Records efficiency metrics for Objective 4**. (Cell 3)\n",
        "\n"
      ],
      "metadata": {
        "id": "vXPViN6jF_UK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Source training and target loaders.\n",
        "\n",
        "\n",
        "\n",
        "import os, time, copy\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "set_global_seed(42)\n",
        "\n",
        "data_root   = \"./data\"\n",
        "num_classes = 10\n",
        "device      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"=== Training source model on CIFAR-10 ===\")\n",
        "source_model = train_source_model(\n",
        "    data_root   = data_root,\n",
        "    num_classes = num_classes,\n",
        "    device      = device,\n",
        "    batch_size  = 128,\n",
        "    epochs      = 50,\n",
        ")\n",
        "\n",
        "base_state = copy.deepcopy(source_model.state_dict())\n",
        "os.makedirs(\"./checkpoints\", exist_ok=True)\n",
        "save_deployed_model(source_model, \"./checkpoints/qmembn_source_cifar10.pth\")\n",
        "\n",
        "print(\"=== Preparing CIFAR-10-C target domain ===\")\n",
        "target_dataset, _ = get_cifar10c_loader(\n",
        "    data_root,\n",
        "    corruption  = \"gaussian_noise\",\n",
        "    severity    = 5,\n",
        "    batch_size  = 64,\n",
        "    num_workers = 2,\n",
        "    shuffle     = False,\n",
        ")\n",
        "\n",
        "support_subset, stream_subset = build_support_and_stream_subsets(\n",
        "    target_dataset,\n",
        "    num_classes     = num_classes,\n",
        "    shots_per_class = 5,\n",
        ")\n",
        "\n",
        "support_loader = DataLoader(support_subset, batch_size=32, shuffle=True)\n",
        "stream_loader  = DataLoader(stream_subset,  batch_size=64, shuffle=False)\n",
        "full_target_loader = DataLoader(target_dataset, batch_size=64, shuffle=True)\n",
        "stream_loader_b1   = DataLoader(stream_subset, batch_size=1, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0HlVy8kq6La",
        "outputId": "a420cca3-cfa3-4664-c3b7-6492c9f3ccc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Training source model on CIFAR-10 ===\n",
            "Best hyperparameters for source training: {'lr': 0.1, 'weight_decay': 0.0001}\n",
            "[Source] Epoch 1/50 Train loss 2.8810 acc 0.2038 Val loss 1.9816 acc 0.2705\n",
            "[Source] Epoch 2/50 Train loss 1.9485 acc 0.2812 Val loss 1.8442 acc 0.3249\n",
            "[Source] Epoch 3/50 Train loss 1.8121 acc 0.3329 Val loss 1.7125 acc 0.3692\n",
            "[Source] Epoch 4/50 Train loss 1.7018 acc 0.3685 Val loss 1.6255 acc 0.4057\n",
            "[Source] Epoch 5/50 Train loss 1.6402 acc 0.3968 Val loss 1.5713 acc 0.4328\n",
            "[Source] Epoch 6/50 Train loss 1.5886 acc 0.4161 Val loss 1.5137 acc 0.4529\n",
            "[Source] Epoch 7/50 Train loss 1.5432 acc 0.4342 Val loss 1.4606 acc 0.4658\n",
            "[Source] Epoch 8/50 Train loss 1.5025 acc 0.4503 Val loss 1.4534 acc 0.4714\n",
            "[Source] Epoch 9/50 Train loss 1.4550 acc 0.4703 Val loss 1.4042 acc 0.4894\n",
            "[Source] Epoch 10/50 Train loss 1.4105 acc 0.4885 Val loss 1.3863 acc 0.4966\n",
            "[Source] Epoch 11/50 Train loss 1.3802 acc 0.4971 Val loss 1.3505 acc 0.5125\n",
            "[Source] Epoch 12/50 Train loss 1.3389 acc 0.5153 Val loss 1.3064 acc 0.5346\n",
            "[Source] Epoch 13/50 Train loss 1.2982 acc 0.5297 Val loss 1.2551 acc 0.5475\n",
            "[Source] Epoch 14/50 Train loss 1.2517 acc 0.5488 Val loss 1.2391 acc 0.5557\n",
            "[Source] Epoch 15/50 Train loss 1.2061 acc 0.5660 Val loss 1.1776 acc 0.5779\n",
            "[Source] Epoch 16/50 Train loss 1.1727 acc 0.5799 Val loss 1.1611 acc 0.5864\n",
            "[Source] Epoch 17/50 Train loss 1.1270 acc 0.5977 Val loss 1.1492 acc 0.5893\n",
            "[Source] Epoch 18/50 Train loss 1.0911 acc 0.6120 Val loss 1.0753 acc 0.6205\n",
            "[Source] Epoch 19/50 Train loss 1.0599 acc 0.6219 Val loss 1.1077 acc 0.6087\n",
            "[Source] Epoch 20/50 Train loss 1.0374 acc 0.6299 Val loss 1.0238 acc 0.6351\n",
            "[Source] Epoch 21/50 Train loss 1.0150 acc 0.6399 Val loss 0.9973 acc 0.6420\n",
            "[Source] Epoch 22/50 Train loss 0.9822 acc 0.6507 Val loss 0.9809 acc 0.6543\n",
            "[Source] Epoch 23/50 Train loss 0.9688 acc 0.6556 Val loss 0.9706 acc 0.6472\n",
            "[Source] Epoch 24/50 Train loss 0.9273 acc 0.6706 Val loss 0.9342 acc 0.6708\n",
            "[Source] Epoch 25/50 Train loss 0.9153 acc 0.6731 Val loss 0.9286 acc 0.6730\n",
            "[Source] Epoch 26/50 Train loss 0.8925 acc 0.6841 Val loss 0.8714 acc 0.6898\n",
            "[Source] Epoch 27/50 Train loss 0.8758 acc 0.6899 Val loss 0.8675 acc 0.6975\n",
            "[Source] Epoch 28/50 Train loss 0.8623 acc 0.6966 Val loss 0.8274 acc 0.7097\n",
            "[Source] Epoch 29/50 Train loss 0.8308 acc 0.7083 Val loss 0.8378 acc 0.7082\n",
            "[Source] Epoch 30/50 Train loss 0.8256 acc 0.7099 Val loss 0.8555 acc 0.6967\n",
            "[Source] Epoch 31/50 Train loss 0.7999 acc 0.7172 Val loss 0.8278 acc 0.7082\n",
            "[Source] Epoch 32/50 Train loss 0.7801 acc 0.7248 Val loss 0.8005 acc 0.7130\n",
            "[Source] Epoch 33/50 Train loss 0.7821 acc 0.7232 Val loss 0.7900 acc 0.7243\n",
            "[Source] Epoch 34/50 Train loss 0.7586 acc 0.7326 Val loss 0.7738 acc 0.7310\n",
            "[Source] Epoch 35/50 Train loss 0.7591 acc 0.7319 Val loss 0.7581 acc 0.7341\n",
            "[Source] Epoch 36/50 Train loss 0.7431 acc 0.7387 Val loss 0.7439 acc 0.7373\n",
            "[Source] Epoch 37/50 Train loss 0.7347 acc 0.7418 Val loss 0.7368 acc 0.7405\n",
            "[Source] Epoch 38/50 Train loss 0.7233 acc 0.7470 Val loss 0.7766 acc 0.7307\n",
            "[Source] Epoch 39/50 Train loss 0.7091 acc 0.7519 Val loss 0.7373 acc 0.7430\n",
            "[Source] Epoch 40/50 Train loss 0.7032 acc 0.7557 Val loss 0.7640 acc 0.7368\n",
            "[Source] Epoch 41/50 Train loss 0.6964 acc 0.7551 Val loss 0.7298 acc 0.7493\n",
            "[Source] Epoch 42/50 Train loss 0.6940 acc 0.7585 Val loss 0.7371 acc 0.7424\n",
            "[Source] Epoch 43/50 Train loss 0.6865 acc 0.7586 Val loss 0.6938 acc 0.7575\n",
            "[Source] Epoch 44/50 Train loss 0.6724 acc 0.7663 Val loss 0.7049 acc 0.7567\n",
            "[Source] Epoch 45/50 Train loss 0.6673 acc 0.7675 Val loss 0.7030 acc 0.7544\n",
            "[Source] Epoch 46/50 Train loss 0.6573 acc 0.7702 Val loss 0.6990 acc 0.7575\n",
            "[Source] Epoch 47/50 Train loss 0.6709 acc 0.7660 Val loss 0.6942 acc 0.7521\n",
            "[Source] Epoch 48/50 Train loss 0.6526 acc 0.7724 Val loss 0.6910 acc 0.7631\n",
            "[Source] Epoch 49/50 Train loss 0.6575 acc 0.7694 Val loss 0.6709 acc 0.7711\n",
            "[Source] Epoch 50/50 Train loss 0.6470 acc 0.7747 Val loss 0.7164 acc 0.7537\n",
            "[Deploy] Saved model to ./checkpoints/qmembn_source_cifar10.pth\n",
            "=== Preparing CIFAR-10-C target domain ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Static baseline, oracle, and clean-stream adaptation.\n",
        "\n",
        "\n",
        "\n",
        "# The static source model (has no target supervision and no adaptation).\n",
        "static_model = QMemResNet18(num_classes=num_classes).to(device)\n",
        "static_model.load_state_dict(base_state)\n",
        "\n",
        "set_qmem_adapt_mode(static_model, adapt=False, use_memory=False)\n",
        "_, static_metrics = evaluate(static_model, stream_loader, device, num_classes)\n",
        "\n",
        "print(\"Static (No Adaptation) metrics on the CIFAR-10-C stream:\", static_metrics)\n",
        "\n",
        "\n",
        "# Fine-tunes the oracle on the full labeled CIFAR-10-C target set.\n",
        "oracle_model = QMemResNet18(num_classes=num_classes).to(device)\n",
        "oracle_model.load_state_dict(base_state)\n",
        "\n",
        "stage1_finetune(\n",
        "    oracle_model,\n",
        "    support_loader=full_target_loader,\n",
        "    device=device,\n",
        "    num_classes=num_classes,\n",
        "    epochs=5,\n",
        "    lr=1e-4,\n",
        ")\n",
        "\n",
        "set_qmem_adapt_mode(oracle_model, adapt=False, use_memory=False)\n",
        "_, oracle_metrics = evaluate(oracle_model, stream_loader, device, num_classes)\n",
        "print(\"Oracle (Target Supervised) Metrics on CIFAR-10-C Stream:\", oracle_metrics)\n",
        "save_deployed_model(oracle_model, \"./checkpoints/qmembn_oracle_cifar10c.pth\")\n",
        "\n",
        "\n",
        "# Q-MemBN's adaptation process on a clean stream (the full method).\n",
        "adapt_model_clean = QMemResNet18(num_classes=num_classes).to(device)\n",
        "adapt_model_clean.load_state_dict(base_state)\n",
        "stage1_finetune(\n",
        "    adapt_model_clean,\n",
        "    support_loader=support_loader,\n",
        "    device=device,\n",
        "    num_classes=num_classes,\n",
        "    epochs=5,\n",
        "    lr=1e-4,\n",
        ")\n",
        "prototype_bank_clean = PrototypeBank(\n",
        "    feature_dim=512,\n",
        "    num_classes=num_classes,\n",
        "    device=device,\n",
        ")\n",
        "prototype_bank_clean.initialize_from_support(adapt_model_clean, support_loader)\n",
        "\n",
        "metrics_qmembn_clean = stage2_adapt_stream(\n",
        "    adapt_model_clean,\n",
        "    prototype_bank   = prototype_bank_clean,\n",
        "    stream_loader    = stream_loader,\n",
        "    device           = device,\n",
        "    num_classes      = num_classes,\n",
        "    lr               = 1e-3,\n",
        "    entropy_fraction = 0.5,\n",
        "    distance_threshold = 2.0,\n",
        "    max_batches      = None,\n",
        ")\n",
        "save_deployed_model(adapt_model_clean, \"./checkpoints/qmembn_adapt_clean_cifar10c.pth\")\n",
        "\n",
        "\n",
        "# Naive adaptation: has the same backbone, but has no entropy/prootype filtering.\n",
        "adapt_model_naive_clean = QMemResNet18(num_classes=num_classes).to(device)\n",
        "adapt_model_naive_clean.load_state_dict(base_state)\n",
        "stage1_finetune(\n",
        "    adapt_model_naive_clean,\n",
        "    support_loader=support_loader,\n",
        "    device=device,\n",
        "    num_classes=num_classes,\n",
        "    epochs=5,\n",
        "    lr=1e-4,\n",
        ")\n",
        "\n",
        "prototype_bank_naive_clean = PrototypeBank(\n",
        "    feature_dim=512,\n",
        "    num_classes=num_classes,\n",
        "    device=device,\n",
        ")\n",
        "\n",
        "prototype_bank_naive_clean.initialize_from_support(adapt_model_naive_clean, support_loader)\n",
        "\n",
        "\n",
        "metrics_naive_clean = stage2_adapt_stream(\n",
        "    adapt_model_naive_clean,\n",
        "    prototype_bank   = prototype_bank_naive_clean,\n",
        "    stream_loader    = stream_loader,\n",
        "    device           = device,\n",
        "    num_classes      = num_classes,\n",
        "    lr               = 1e-3,\n",
        "    entropy_fraction = 1.0, # see: entropy filtering.\n",
        "    distance_threshold = 1e9, # see: prototype filtering.\n",
        "    max_batches      = None,\n",
        ")\n",
        "\n",
        "save_deployed_model(adapt_model_naive_clean, \"./checkpoints/qmembn_naive_clean_cifar10c.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "489d78ea-2bd2-4a36-c949-d69c491d9d51",
        "id": "sVZC2z45A91D"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Static (No Adaptation) metrics on the CIFAR-10-C stream: {'accuracy': 0.6590954773869346, 'precision_macro': 0.6696963906288147, 'recall_macro': 0.6590954661369324, 'f1_macro': 0.6576816439628601, 'rmse': 0.21556910872459412}\n",
            "[Stage I] Epoch 1/5, loss 1.1290\n",
            "[Stage I] Epoch 2/5, loss 1.0829\n",
            "[Stage I] Epoch 3/5, loss 1.0673\n",
            "[Stage I] Epoch 4/5, loss 1.0531\n",
            "[Stage I] Epoch 5/5, loss 1.0447\n",
            "Oracle (target-supervised) metrics on CIFAR-10-C stream: {'accuracy': 0.7, 'precision_macro': 0.6992928981781006, 'recall_macro': 0.699999988079071, 'f1_macro': 0.6992831230163574, 'rmse': 0.20176437497138977}\n",
            "[Deploy] Saved model to ./checkpoints/qmembn_oracle_cifar10c.pth\n",
            "[Stage I] Epoch 1/5, loss 1.5752\n",
            "[Stage I] Epoch 2/5, loss 1.5222\n",
            "[Stage I] Epoch 3/5, loss 1.6831\n",
            "[Stage I] Epoch 4/5, loss 1.6728\n",
            "[Stage I] Epoch 5/5, loss 1.6601\n",
            "[Stage II] Final metrics on adapted stream: {'accuracy': 0.6678391959798995, 'precision_macro': 0.6743624806404114, 'recall_macro': 0.6678391695022583, 'f1_macro': 0.666246771812439, 'rmse': 0.2124655842781067}\n",
            "[Deploy] Saved model to ./checkpoints/qmembn_adapt_clean_cifar10c.pth\n",
            "[Stage I] Epoch 1/5, loss 1.5850\n",
            "[Stage I] Epoch 2/5, loss 1.6796\n",
            "[Stage I] Epoch 3/5, loss 1.6381\n",
            "[Stage I] Epoch 4/5, loss 1.5027\n",
            "[Stage I] Epoch 5/5, loss 1.5285\n",
            "[Stage II] Final metrics on adapted stream: {'accuracy': 0.6662311557788945, 'precision_macro': 0.671379566192627, 'recall_macro': 0.6662311553955078, 'f1_macro': 0.6635282635688782, 'rmse': 0.21326902508735657}\n",
            "[Deploy] Saved model to ./checkpoints/qmembn_naive_clean_cifar10c.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Robustness to malicious inputs, latency, and objective summaries.\n",
        "\n",
        "\n",
        "\n",
        "class PoisonedStreamDataset(Dataset):\n",
        "    def __init__(self, base_subset, poison_fraction=0.2):\n",
        "        self.base_subset = base_subset\n",
        "        self.n = len(base_subset)\n",
        "        num_poison = int(self.n * poison_fraction)\n",
        "        perm = torch.randperm(self.n)\n",
        "        self.poison_idx = set(perm[:num_poison].tolist())\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.base_subset[idx]\n",
        "        if idx in self.poison_idx:\n",
        "            noise = torch.randn_like(img) * 2.5\n",
        "            img = img + noise\n",
        "        return img, label\n",
        "\n",
        "poisoned_stream_dataset = PoisonedStreamDataset(stream_subset, poison_fraction=0.2)\n",
        "poisoned_stream_loader  = DataLoader(poisoned_stream_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# Q-MemBN+ on a poisoned stream. How does it do?\n",
        "adapt_model_poison = QMemResNet18(num_classes=num_classes).to(device)\n",
        "adapt_model_poison.load_state_dict(base_state)\n",
        "stage1_finetune(\n",
        "    adapt_model_poison,\n",
        "    support_loader=support_loader,\n",
        "    device=device,\n",
        "    num_classes=num_classes,\n",
        "    epochs=5,\n",
        "    lr=1e-4,\n",
        ")\n",
        "prototype_bank_poison = PrototypeBank(\n",
        "    feature_dim=512,\n",
        "    num_classes=num_classes,\n",
        "    device=device,\n",
        ")\n",
        "prototype_bank_poison.initialize_from_support(adapt_model_poison, support_loader)\n",
        "\n",
        "metrics_qmembn_poison = stage2_adapt_stream(\n",
        "    adapt_model_poison,\n",
        "    prototype_bank   = prototype_bank_poison,\n",
        "    stream_loader    = poisoned_stream_loader,\n",
        "    device           = device,\n",
        "    num_classes      = num_classes,\n",
        "    lr               = 1e-3,\n",
        "    entropy_fraction = 0.5,\n",
        "    distance_threshold = 2.0,\n",
        "    max_batches      = None,\n",
        ")\n",
        "\n",
        "\n",
        "# Naive adaptation on poisoned stream.\n",
        "adapt_model_naive_poison = QMemResNet18(num_classes=num_classes).to(device)\n",
        "adapt_model_naive_poison.load_state_dict(base_state)\n",
        "stage1_finetune(\n",
        "    adapt_model_naive_poison,\n",
        "    support_loader=support_loader,\n",
        "    device=device,\n",
        "    num_classes=num_classes,\n",
        "    epochs=5,\n",
        "    lr=1e-4,\n",
        ")\n",
        "prototype_bank_naive_poison = PrototypeBank(\n",
        "    feature_dim=512,\n",
        "    num_classes=num_classes,\n",
        "    device=device,\n",
        ")\n",
        "prototype_bank_naive_poison.initialize_from_support(adapt_model_naive_poison, support_loader)\n",
        "\n",
        "metrics_naive_poison = stage2_adapt_stream(\n",
        "    adapt_model_naive_poison,\n",
        "    prototype_bank   = prototype_bank_naive_poison,\n",
        "    stream_loader    = poisoned_stream_loader,\n",
        "    device           = device,\n",
        "    num_classes      = num_classes,\n",
        "    lr               = 1e-3,\n",
        "    entropy_fraction = 1.0,\n",
        "    distance_threshold = 1e9,\n",
        "    max_batches      = None,\n",
        ")\n",
        "\n",
        "\n",
        "# Model size (for my Objective 4).\n",
        "param_model = QMemResNet18(num_classes=num_classes)\n",
        "total_params = sum(p.numel() for p in param_model.parameters())\n",
        "total_params_m = total_params / 1e6\n",
        "\n",
        "\n",
        "# Latency for Stage-II adaptation at B=1 (for my Objective 4).\n",
        "lat_model = QMemResNet18(num_classes=num_classes).to(device)\n",
        "lat_model.load_state_dict(base_state)\n",
        "stage1_finetune(\n",
        "    lat_model,\n",
        "    support_loader=support_loader,\n",
        "    device=device,\n",
        "    num_classes=num_classes,\n",
        "    epochs=1,\n",
        "    lr=1e-4,\n",
        ")\n",
        "\n",
        "\n",
        "prototype_bank_lat = PrototypeBank(\n",
        "    feature_dim=512,\n",
        "    num_classes=num_classes,\n",
        "    device=device,\n",
        ")\n",
        "prototype_bank_lat.initialize_from_support(lat_model, support_loader)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.synchronize()\n",
        "t0 = time.perf_counter()\n",
        "_ = stage2_adapt_stream(\n",
        "    lat_model,\n",
        "    prototype_bank   = prototype_bank_lat,\n",
        "    stream_loader    = stream_loader_b1,\n",
        "    device           = device,\n",
        "    num_classes      = num_classes,\n",
        "    lr               = 1e-3,\n",
        "    entropy_fraction = 0.5,\n",
        "    distance_threshold = 2.0,\n",
        "    max_batches      = 50,\n",
        ")\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.synchronize()\n",
        "t1 = time.perf_counter()\n",
        "avg_latency_ms = (t1 - t0) / 50.0 * 1000.0\n",
        "\n",
        "\n",
        "# Summaries\n",
        "acc_static = static_metrics[\"accuracy\"]\n",
        "acc_adapt  = metrics_qmembn_clean[\"accuracy\"]\n",
        "acc_oracle = oracle_metrics[\"accuracy\"]\n",
        "\n",
        "print(\"\\n=== Objective 1: Static vs Q-MemBN+ on CIFAR-10-C ===\")\n",
        "print(f\"Static accuracy:      {acc_static:.4f}\")\n",
        "print(f\"Adapted accuracy:     {acc_adapt:.4f}\")\n",
        "print(f\"Absolute gain:        {(acc_adapt - acc_static)*100:.2f} percentage points\")\n",
        "\n",
        "gap_static = acc_oracle - acc_static\n",
        "gap_adapt  = acc_oracle - acc_adapt\n",
        "if gap_static > 0:\n",
        "    gap_reduction_pct = (1.0 - gap_adapt / gap_static) * 100.0\n",
        "else:\n",
        "    gap_reduction_pct = float(\"nan\")\n",
        "\n",
        "print(\"\\n=== Objective 2: Gap to oracle (CIFAR-10-C) ===\")\n",
        "print(f\"Oracle accuracy:      {acc_oracle*100:.2f} %\")\n",
        "print(f\"Gap (oracle - static):  {gap_static*100:.2f} pp\")\n",
        "print(f\"Gap (oracle - adapted): {gap_adapt*100:.2f} pp\")\n",
        "print(f\"Gap reduction:          {gap_reduction_pct:.2f} %\")\n",
        "\n",
        "acc_naive_clean   = metrics_naive_clean[\"accuracy\"]\n",
        "acc_naive_poison  = metrics_naive_poison[\"accuracy\"]\n",
        "acc_qmembn_clean  = metrics_qmembn_clean[\"accuracy\"]\n",
        "acc_qmembn_poison = metrics_qmembn_poison[\"accuracy\"]\n",
        "\n",
        "drop_naive  = (acc_naive_clean  - acc_naive_poison)  * 100.0\n",
        "drop_qmembn = (acc_qmembn_clean - acc_qmembn_poison) * 100.0\n",
        "\n",
        "print(\"\\n=== Objective 3: Robustness to 20% poisoned stream ===\")\n",
        "print(f\"Naive adaptation drop:   {drop_naive:.2f} percentage points\")\n",
        "print(f\"Q-MemBN+ drop:           {drop_qmembn:.2f} percentage points\")\n",
        "\n",
        "print(\"\\n=== Objective 4: Efficiency ===\")\n",
        "print(f\"Model size:              {total_params_m:.2f}M parameters\")\n",
        "print(f\"Stage-II latency (B=1):  {avg_latency_ms:.2f} ms per batch\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuoM-kMHDqIZ",
        "outputId": "4891d56d-b0c4-412e-f611-f6ad4b402614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Stage I] Epoch 1/5, loss 1.5399\n",
            "[Stage I] Epoch 2/5, loss 1.6663\n",
            "[Stage I] Epoch 3/5, loss 1.4609\n",
            "[Stage I] Epoch 4/5, loss 1.5733\n",
            "[Stage I] Epoch 5/5, loss 1.3674\n",
            "[Stage II] Final metrics on adapted stream: {'accuracy': 0.5516582914572864, 'precision_macro': 0.6218993663787842, 'recall_macro': 0.5516583323478699, 'f1_macro': 0.564264714717865, 'rmse': 0.26018691062927246}\n",
            "[Stage I] Epoch 1/5, loss 1.6842\n",
            "[Stage I] Epoch 2/5, loss 1.5306\n",
            "[Stage I] Epoch 3/5, loss 1.6029\n",
            "[Stage I] Epoch 4/5, loss 1.3970\n",
            "[Stage I] Epoch 5/5, loss 1.6983\n",
            "[Stage II] Final metrics on adapted stream: {'accuracy': 0.5458291457286432, 'precision_macro': 0.6163973808288574, 'recall_macro': 0.545829176902771, 'f1_macro': 0.5571845173835754, 'rmse': 0.25905847549438477}\n",
            "[Stage I] Epoch 1/1, loss 1.8463\n",
            "[Stage II] Final metrics on adapted stream: {'accuracy': 0.36, 'precision_macro': 0.40734848380088806, 'recall_macro': 0.36718255281448364, 'f1_macro': 0.3135073482990265, 'rmse': 0.2699425220489502}\n",
            "\n",
            "=== Objective 1: Static vs Q-MemBN+ on CIFAR-10-C ===\n",
            "Static accuracy:      0.6591\n",
            "Adapted accuracy:     0.6678\n",
            "Absolute gain:        0.87 percentage points\n",
            "\n",
            "=== Objective 2: Gap to oracle (CIFAR-10-C) ===\n",
            "Oracle accuracy:      70.00 %\n",
            "Gap (oracle - static):  4.09 pp\n",
            "Gap (oracle - adapted): 3.22 pp\n",
            "Gap reduction:          21.35 %\n",
            "\n",
            "=== Objective 3: Robustness to 20% poisoned stream ===\n",
            "Naive adaptation drop:   12.20 percentage points\n",
            "Q-MemBN+ drop:           11.46 percentage points\n",
            "\n",
            "=== Objective 4: Efficiency ===\n",
            "Model size:              11.17M parameters\n",
            "Stage-II latency (B=1):  28.08 ms per batch\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}